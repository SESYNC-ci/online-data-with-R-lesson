---
---

## Web Scraping

That "http" at the beginning of the URL for a possible data source is
a protocol---an understanding between a client and a server about how
to communicate. The client could either be a web browser such as 
Chrome or Firefox, or your web scraping program written in R, 
as long as it uses the correct protocol. 
After all, [servers exist to serve](https://xkcd.com/869/).

===

The following example
uses the [httr](){:.rlib} and [rvest](){:.rlib} packages to issue a 
HTTP request and handle the response. 

The page we are scraping, <http://research.jisao.washington.edu/pdo/PDO.latest>,
deals with the [Pacific Decadal Oscillation](https://en.wikipedia.org/wiki/Pacific_decadal_oscillation) 
(PDO), a periodic switching between
warm and cool water temperatures in the northern Pacific Ocean. Specifically, it
contains monthly values from 1900-2018 indicating how far above or below normal the sea surface
temperature across the northern Pacific Ocean was during that month.
{:.notes}

```{r, handout = 0, message = FALSE}
library(httr)

response <- GET('http://research.jisao.washington.edu/pdo/PDO.latest')
response
```

The `GET()` function from [httr](){:.rlib} can be used with a single argument, 
a text string with the URL of the page you are scraping.
{:.notes}

===

The response is binary (0s and 1s). The [rvest](){:.rlib} package translates
the raw content into an HTML document, just like a browser does. We use the 
`read_html` function to do this.

```{r, handout = 0, message = FALSE}
library(rvest) 

pdo_doc <- read_html(response)
pdo_doc
```

The HTML document returned by `read_html` is no longer 0s and 1s, it now
contains readable text. However it is stored as a single long character string.
We need to do some additional processing to make it useful.
{:.notes}

===

If you look at the HTML document, you can see that all the data is inside an 
element called `"p"`. We use the `html_node` function to extract the 
single `"p"` element from the HTML document, then the `html_text` function
to extract the text from that element.

```{r, handout = 0}
pdo_node <- html_node(pdo_doc, "p")
pdo_text <- html_text(pdo_node)
```

The first argument of `html_node` is the HTML document, and the second
argument is the name of the element we want to extract. `html_text` 
takes the extracted element as input.
{:.notes}

===

Now we have a long text string containing all the data. We can use text mining tools
like regular expressions to pull out data. If we want the twelve monthly
values for the year 2017, we can use the [stringr](){:.rlib} package to get 
all the text between the strings "2017" and "2018" with `str_match`.

```{r, handout = 0, message = FALSE}
library(stringr)
pdo_text_2017 <- str_match(pdo_text, "(?<=2017).*.(?=\\n2018)")
```

===

Then extract all the numeric values in the substring with `str_extract_all`.

```{r, handout = 0}
str_extract_all(pdo_text_2017[1], "[0-9-.]+")
```

You can learn more about how to use regular expressions to extract information
from text strings in [SESYNC's text mining lesson](https://cyberhelp.sesync.org/text-mining-lesson/).
{:.notes}

===

## Manual web scraping is hard!

Pages designed for humans are increasingly harder to parse programmatically.

- Servers provide different responses based on client "metadata"
- JavaScript often needs to be executed by the client
- The HTML `<table>` is drifting into obscurity (mostly for the better)

===

## HTML Tables

Sites with easily accessible HTML tables nowadays may be specifically
intended to be parsed programmatically, rather than browsed by a human reader.
The US Census provides some documentation for their data services in a massive table:

<https://api.census.gov/data/2017/acs/acs5/variables.html>

===

`html_table()` converts the HTML table into an R 
data frame. Set `fill = TRUE` so that inconsistent numbers 
of columns in each row are filled in.

```{r, handout = 0}
census_vars_doc <- read_html('https://api.census.gov/data/2017/acs/acs5/variables.html')

table_raw <- html_node(census_vars_doc, 'table')

# This line takes a few moments to run.
census_vars <- html_table(table_raw, fill = TRUE) 
```

```{r}
head(census_vars)
```

===

We can use our tidy data tools to search this unwieldy
documentation for variables of interest.

The call to `set_tidy_names()` is necessary because the table
extraction results in some columns with undefined names---a
common occurrence when parsing Web content. Next, we use `select()`
to select only the `Name` and `Label` columns, and `filter()`
to select only the rows where the `Label` column contains the 
substring `"Median household income"`. The `grepl()` function
allows us to filter by a regular expression.
{:.notes}

```{r, handout = 0, message = FALSE}
library(tidyverse)

census_vars %>%
  set_tidy_names() %>%
  select(Name, Label) %>%
  filter(grepl('Median household income', Label))
```
